{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d62e5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Basmala Kamal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"RJT1990/GeneralThoughtArchive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf56b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f819da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   question_id                                       question_url  \\\n",
      "0       806845  https://gr.inc/question/how-do-the-neural-resp...   \n",
      "1      1730456  https://gr.inc/question/lets-consider-some-arr...   \n",
      "2      3236068  https://gr.inc/question/what-are-the-primary-c...   \n",
      "3      1717140  https://gr.inc/question/consider-a-football-to...   \n",
      "4      3235833  https://gr.inc/question/given-the-context-of-h...   \n",
      "\n",
      "                                            question  \\\n",
      "0     How do the neural respiratory centers operate?   \n",
      "1  Let's consider some array A. The following alg...   \n",
      "2  What are the primary criticisms Aristotle rais...   \n",
      "3  Consider a football tournament where n teams p...   \n",
      "4  Given the context of Heidegger's philosophy, p...   \n",
      "\n",
      "                                    reference_answer prev_messages  \\\n",
      "0  In the medulla oblongata, respiratory neurons ...          None   \n",
      "1                                               None          None   \n",
      "2  Aristotle's criticisms of Plato's theory of fo...          None   \n",
      "3                                               None          None   \n",
      "4  The anticipation of death as a possibility is ...          None   \n",
      "\n",
      "             model_name                                       model_answer  \\\n",
      "0  DeepSeek/DeepSeek-R1  The neural respiratory centers operate through...   \n",
      "1  DeepSeek/DeepSeek-R1  ```python\\nimport sys\\nfrom collections import...   \n",
      "2  DeepSeek/DeepSeek-R1  Aristotle's criticisms of Plato's Theory of Fo...   \n",
      "3  DeepSeek/DeepSeek-R1  ```python\\nfrom collections import Counter\\n\\n...   \n",
      "4  DeepSeek/DeepSeek-R1  In Heidegger's philosophy, particularly throug...   \n",
      "\n",
      "                                     model_reasoning               task  \\\n",
      "0  Okay, so I need to figure out how the neural r...      Human Biology   \n",
      "1  Okay, let's see. I need to find the maximum po...  Greedy algorithms   \n",
      "2  Okay, so I need to figure out what Aristotle's...         Philosophy   \n",
      "3  Okay, let's see. I need to solve this programm...    Complete search   \n",
      "4  Okay, so I need to explain how anticipating de...         Philosophy   \n",
      "\n",
      "  question_license         question_source  community_answer_score  \\\n",
      "0              MIT            General/VNet                       0   \n",
      "1       Apache-2.0               BAAI/TACO                       0   \n",
      "2  CC-BY-NC-SA-4.0  Meta/natural_reasoning                       0   \n",
      "3       Apache-2.0               BAAI/TACO                       0   \n",
      "4  CC-BY-NC-SA-4.0  Meta/natural_reasoning                       0   \n",
      "\n",
      "   community_question_score  verifier_score  \n",
      "0                         0             NaN  \n",
      "1                         0             NaN  \n",
      "2                         0             NaN  \n",
      "3                         0             NaN  \n",
      "4                         0             NaN  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(ds['train'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791f2aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(430788, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776da16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 430788 entries, 0 to 430787\n",
      "Data columns (total 14 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   question_id               430788 non-null  int64  \n",
      " 1   question_url              430788 non-null  object \n",
      " 2   question                  430788 non-null  object \n",
      " 3   reference_answer          306178 non-null  object \n",
      " 4   prev_messages             102234 non-null  object \n",
      " 5   model_name                430788 non-null  object \n",
      " 6   model_answer              430788 non-null  object \n",
      " 7   model_reasoning           428151 non-null  object \n",
      " 8   task                      430788 non-null  object \n",
      " 9   question_license          429474 non-null  object \n",
      " 10  question_source           430788 non-null  object \n",
      " 11  community_answer_score    430788 non-null  int64  \n",
      " 12  community_question_score  430788 non-null  int64  \n",
      " 13  verifier_score            338377 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(10)\n",
      "memory usage: 46.0+ MB\n",
      "None\n",
      "\n",
      "Column names:\n",
      "['question_id', 'question_url', 'question', 'reference_answer', 'prev_messages', 'model_name', 'model_answer', 'model_reasoning', 'task', 'question_license', 'question_source', 'community_answer_score', 'community_question_score', 'verifier_score']\n"
     ]
    }
   ],
   "source": [
    "# Check data types, non-null counts, memory usage\n",
    "print(df.info())\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0b8926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "question_id                      0\n",
      "question_url                     0\n",
      "question                         0\n",
      "reference_answer            124610\n",
      "prev_messages               328554\n",
      "model_name                       0\n",
      "model_answer                     0\n",
      "model_reasoning               2637\n",
      "task                             0\n",
      "question_license              1314\n",
      "question_source                  0\n",
      "community_answer_score           0\n",
      "community_question_score         0\n",
      "verifier_score               92411\n",
      "dtype: int64\n",
      "\n",
      "Missing value percentages:\n",
      "question_id                  0.000000\n",
      "question_url                 0.000000\n",
      "question                     0.000000\n",
      "reference_answer            28.926061\n",
      "prev_messages               76.268141\n",
      "model_name                   0.000000\n",
      "model_answer                 0.000000\n",
      "model_reasoning              0.612134\n",
      "task                         0.000000\n",
      "question_license             0.305022\n",
      "question_source              0.000000\n",
      "community_answer_score       0.000000\n",
      "community_question_score     0.000000\n",
      "verifier_score              21.451619\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Missing value analysis\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nMissing value percentages:\")\n",
    "print((df.isnull().sum() / len(df)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "722b1bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of model_reasoning (first 3 rows):\n",
      "\n",
      "--- Row 0 ---\n",
      "Okay, so I need to figure out how the neural respiratory centers work. Let me start by recalling what I know about breathing and the nervous system. I remember that breathing is controlled by parts of the brain, right? There's the medulla and the pons mentioned in some biology classes. But wait, how exactly do they work together?\n",
      "\n",
      "First, there's the medulla oblongata. I think that's the primary control center. It has groups of neurons called the dorsal respiratory group and the ventral respirato\n",
      "\n",
      "--- Row 1 ---\n",
      "Okay, let's see. I need to find the maximum possible force of an array after reordering some elements. The force is calculated by summing the squares of the lengths of consecutive equal elements. So the problem is, given the array generated by a specific recurrence relation, how can I rearrange its elements to maximize this sum of squares?\n",
      "\n",
      "Hmm. The key here is that the optimal arrangement groups as many same elements as possible into the largest possible blocks. Because when you square the leng\n",
      "\n",
      "--- Row 2 ---\n",
      "Okay, so I need to figure out what Aristotle's main criticisms were against Plato's Theory of Forms and how these criticisms show their different views on metaphysics, reality, and knowledge. Let me start by recalling what I know about Plato's Theory of Forms. From what I remember, Plato believed that non-material abstract forms (or ideas) are the most accurate reality. These forms are perfect, unchanging, and exist in a separate realm from the physical world we experience. For example, there's \n"
     ]
    }
   ],
   "source": [
    "# Check if critical text fields are populated\n",
    "print(\"\\nSample of model_reasoning (first 3 rows):\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(df.iloc[i]['model_reasoning'][:500])  # First 500 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c211825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model reasoning missing:\n",
      "0\n",
      "\n",
      "Dataset size after removing missing reasoning: (430788, 14)\n"
     ]
    }
   ],
   "source": [
    "# Check reasoning field completeness\n",
    "print(\"\\nModel reasoning missing:\")\n",
    "print(df['model_answer'].isnull().sum())\n",
    "\n",
    "# Drop rows without reasoning (can't extract features)\n",
    "df_clean = df[df['model_answer'].notna()].copy()\n",
    "print(f\"\\nDataset size after removing missing reasoning: {df_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7bd1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Token count\n",
    "df_clean['feat_token_count'] = df_clean['model_answer'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Sentence count (periods, exclamation, question marks)\n",
    "df_clean['feat_sentence_count'] = df_clean['model_answer'].apply(\n",
    "    lambda x: len(re.findall(r'[.!?]+', str(x)))\n",
    ")\n",
    "\n",
    "# Step markers (1., 2., Step 1, etc.)\n",
    "df_clean['feat_step_markers'] = df_clean['model_answer'].apply(\n",
    "    lambda x: len(re.findall(r'\\b(?:step\\s*\\d+|^\\d+\\.|\\n\\d+\\.)', str(x).lower()))\n",
    ")\n",
    "\n",
    "# Average sentence length\n",
    "df_clean['feat_avg_sentence_len'] = (df_clean['feat_token_count'] / \n",
    "                                      (df_clean['feat_sentence_count'] + 1))\n",
    "\n",
    "# Max sentence length (split by periods, find longest)\n",
    "def max_sent_len(text):\n",
    "    sentences = re.split(r'[.!?]+', str(text))\n",
    "    if not sentences:\n",
    "        return 0\n",
    "    return max(len(s.split()) for s in sentences)\n",
    "\n",
    "df_clean['feat_max_sentence_len'] = df_clean['model_answer'].apply(max_sent_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f256167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaTeX operator counts (sum, frac, etc.)\n",
    "df_clean['feat_latex_operators'] = df_clean['model_answer'].apply(\n",
    "    lambda x: len(re.findall(r'\\\\(?:frac|sum|int|times|div|sqrt)', str(x)))\n",
    ")\n",
    "\n",
    "# Digit ratio (proportion of characters that are digits)\n",
    "df_clean['feat_digit_ratio'] = df_clean['model_answer'].apply(\n",
    "    lambda x: sum(c.isdigit() for c in str(x)) / (len(str(x)) + 1)\n",
    ")\n",
    "\n",
    "# Equation sign density (=, +, -, *, /)\n",
    "df_clean['feat_equation_signs'] = df_clean['model_answer'].apply(\n",
    "    lambda x: len(re.findall(r'[=+\\-*/]', str(x)))\n",
    ")\n",
    "\n",
    "# Operator variety (unique math operators)\n",
    "def operator_variety(text):\n",
    "    operators = set(re.findall(r'[+\\-*/=<>≤≥]', str(text)))\n",
    "    return len(operators)\n",
    "\n",
    "df_clean['feat_operator_variety'] = df_clean['model_answer'].apply(operator_variety)\n",
    "\n",
    "# Code block presence (``````)\n",
    "df_clean['feat_has_code_block'] = df_clean['model_answer'].apply(\n",
    "    lambda x: 1 if '```' in str(x) else 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connective words (because, therefore, thus, so, hence)\n",
    "logic_connectives = ['because', 'therefore', 'thus', 'hence', 'since', 'consequently']\n",
    "df_clean['feat_logic_connectives'] = df_clean['model_answer'].apply(\n",
    "    lambda x: sum(str(x).lower().count(word) for word in logic_connectives)\n",
    ")\n",
    "\n",
    "# Self-correction markers (wait, actually, correction, mistake)\n",
    "correction_words = ['wait', 'actually', 'correction', 'mistake', 'error', 'wrong']\n",
    "df_clean['feat_self_corrections'] = df_clean['model_answer'].apply(\n",
    "    lambda x: sum(str(x).lower().count(word) for word in correction_words)\n",
    ")\n",
    "\n",
    "# Contradiction indicators (but, however near numbers)\n",
    "def contradictions_near_numbers(text):\n",
    "    text_lower = str(text).lower()\n",
    "    # Find \"but\" or \"however\" within 10 words of a number\n",
    "    matches = re.findall(r'(?:\\d+.{0,50}(?:but|however))|(?:(?:but|however).{0,50}\\d+)', text_lower)\n",
    "    return len(matches)\n",
    "\n",
    "df_clean['feat_contradiction_markers'] = df_clean['model_answer'].apply(contradictions_near_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question-to-reasoning overlap (Jaccard similarity)\n",
    "def jaccard_similarity(text1, text2):\n",
    "    set1 = set(str(text1).lower().split())\n",
    "    set2 = set(str(text2).lower().split())\n",
    "    if not set1 or not set2:\n",
    "        return 0\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "df_clean['feat_question_overlap'] = df_clean.apply(\n",
    "    lambda row: jaccard_similarity(row['question'], row['model_reasoning']), axis=1\n",
    ")\n",
    "\n",
    "# Extra numbers (numbers in reasoning not in question)\n",
    "def extract_numbers(text):\n",
    "    return set(re.findall(r'\\d+\\.?\\d*', str(text)))\n",
    "\n",
    "df_clean['feat_extra_numbers'] = df_clean.apply(\n",
    "    lambda row: len(extract_numbers(row['model_reasoning']) - extract_numbers(row['question'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Final answer formatting (presence of \"Answer:\", \"Final answer:\", etc.)\n",
    "df_clean['feat_has_answer_marker'] = df_clean['model_reasoning'].apply(\n",
    "    lambda x: 1 if re.search(r'\\b(?:answer|conclusion|result):', str(x).lower()) else 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type-token ratio (unique words / total words)\n",
    "def type_token_ratio(text):\n",
    "    words = str(text).lower().split()\n",
    "    if not words:\n",
    "        return 0\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "df_clean['feat_type_token_ratio'] = df_clean['model_reasoning'].apply(type_token_ratio)\n",
    "\n",
    "# Punctuation density\n",
    "df_clean['feat_punctuation_density'] = df_clean['model_reasoning'].apply(\n",
    "    lambda x: len(re.findall(r'[.,;:!?]', str(x))) / (len(str(x)) + 1)\n",
    ")\n",
    "\n",
    "# Parentheses balance (are they properly matched?)\n",
    "def parentheses_balanced(text):\n",
    "    count = 0\n",
    "    for char in str(text):\n",
    "        if char == '(':\n",
    "            count += 1\n",
    "        elif char == ')':\n",
    "            count -= 1\n",
    "        if count < 0:\n",
    "            return 0  # Unbalanced\n",
    "    return 1 if count == 0 else 0\n",
    "\n",
    "df_clean['feat_parentheses_balanced'] = df_clean['model_reasoning'].apply(parentheses_balanced)\n",
    "\n",
    "# Repeated n-gram rate (3-grams that appear more than once)\n",
    "def repeated_trigrams(text):\n",
    "    words = str(text).lower().split()\n",
    "    if len(words) < 3:\n",
    "        return 0\n",
    "    trigrams = [' '.join(words[i:i+3]) for i in range(len(words)-2)]\n",
    "    if not trigrams:\n",
    "        return 0\n",
    "    unique_trigrams = len(set(trigrams))\n",
    "    return 1 - (unique_trigrams / len(trigrams))\n",
    "\n",
    "df_clean['feat_repeated_trigrams'] = df_clean['model_reasoning'].apply(repeated_trigrams)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
